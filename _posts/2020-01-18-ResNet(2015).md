---
layout: post
title:  "ResNet(2015)"
date:   2020-01-18 20:10:13
categories: Deep_Learning
---



# ResNet-50 (2015)

- *Deep Residual Learning for Image Recognition* - Kaiming He, Xiangyu Zhang etc..



#### 특징

- 마이크로소프트에서 개발한 알고리즘.
- GoogLeNet의 layer(22 layers)보다 훨씬 많은 layer로 구성되어 있다(152 layers).
- 기본적인 Architecture는 VGG-19의 뼈대를 사용한다. 여기에 convolution layer를 추가하여 depth를 늘린 후 shortcut을 추가하여 모델을 만든다.



#### Residual Block

Convolution layer를 증가시키면 특성이 복잡해지고 더 정교하게 훈련을 할 수 있지만, 그렇다고 해서 모델의 성능이 좋아지는 것은 아니다. 이는 논문에서 실험을 통해 overfitting 현상에서 초래된 결과가 아니라는 것이 증명이 되었다.

논문의 저자들은 convolution layer들과 fully connected layer들로 20층의 Network와 56층의 Network를 각각 만든 다음에 성능을 테스트 하였다. 그 결과는 아래 사진과 같은데, layer가 증가했지만 training error도 증가한 것을 볼 수 있다.

![trainning error without residual blocks](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/trainning error without residual blocks.png)

> 앝은 layer를 가진 모델과 그 모델을 여러번 쌓아서 만든 깊은 layer를 가진 모델 을 훈련시키고 성능 테스트를 해본 결과 더 깊은 layer를 가진 모델쪽에서  training error가 발견되었다. 만일 overfitting 현상이 발생했다면, 훈련 데이터에서의 성능이 좋아지고 테스트 데이터에서 성능이 저하되어야 하는데 오히려 깊은 layer를 가진 모델에서 훈련 데이터의 성능이 떨어진 것이다.

ResNet은 깊은 layer로 모델을 구성하면서 생길 수 있는 문제점을 Residual Block으로 방지했다. Residual Block의 구조는 아래 그림의 오른쪽 부분에서 확인할 수 있다.

![Residual Block Architecture](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/Residual block architecture.png)

기존의 망은 input x에 대한 target y로 mapping하는 함수 H(x)를 찾는 것이 목표였다. Residual Block은 기존의 망과 다르게 입력값을 출력값에 더해줄 수 있도록 지름길(shortcut)을 하나 만들어주는 것이다. 이때 H(x)는 입력 데이터가 ReLU함수를 통과하여 나온 f(x)에 입력값 x를 더해준 것이 된다.
$$
H(x) = f(x)+x\\
$$
여기서 x는 입력 데이터로 현 시점에서는 변할 수 없는 값이다. 즉 ResNet에서는 H(x)를 최소화 하는 것이 목표이기 때문에 f(x)를 최소로 만들어야 한다는 것이고, 이를 다시말하면 우변의 x값을 좌변으로 이항했을 때의 식인 $$H(x)-x$$가 최소가 되어야 한다는 것이다. 여기서 $$H(x)-x$$를 residual(잔여물, 잔차)라고 한다. Network의 이름인 ResNet에서 Res는 residual을 의미한다.

논문의 저자들은 Residual block, 즉 shortcut들이 효과가 있는지를 확인하기 위해 이미지넷에서 18층 및 34층의 plain Network와 ResNet의 성능을 비교했다. 성능 비교 결과 residual block을 사용한 훈련에서 망이 깊을수록 error도 작아지는 것을 볼 수 있었고, 이는 Network에 shortcut을 연결해서 residual을 최소가 되게 학습한 효과가 있다는 것을 알 수 있다.

![testing residual block](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/testing residual block.png)



#### 성능

그렇다면 layer의 수를 어느정도 늘렸을 때 성능이 가장 뛰어나다고 할 수 있을까?

논문의 저자들은 각각 18층, 34층, 50층, 101층, 152층의 ResNet을 만들어 테스트하고 layer 수에 따른 성능을 분석하였다. 그 결과 Network가 깊은 구조일수록 성능도 좋다는 결론에 도달하였다. 즉 152층의 ResNet이 가장 뛰어난 성능을 보여주었다.

![testing appropriate number of layers](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/testing appropriate number of layers.png)

