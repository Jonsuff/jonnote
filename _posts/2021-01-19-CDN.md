---
layout: post
title:  "CDN"
date:   2021-01-19 13:05:00
categories: Deep_Learning
---



# Continuous Disparity Network(CDN)

| 제목 |    Wasserstein Distances for Stereo Disparity Estimation     |
| ---- | :----------------------------------------------------------: |
| 저자 | Divyansh Garg, Yan Wang, Bharath Hariharan, Mark Campbell, Kilian Q. Weinberger |
| 출판 |                          arXiv 2020                          |



## Introduction

일반적으로 stereo 이미지를 이용하여 Disparity를 예측하는 모델들은 픽셀 좌표를 가지고 계산을 하기때문에 disparity 값이 정수로 나오게 되고, 결과적으로 실제 값과 오차가 생기게 된다. 오차가 있는 disparity로 depth를 예측하게 되면, 그 depth값도 실제값과는 다른 값이기 때문에 정확한 depth를 예측하기 어렵다.

본 논문의 저자는 학습 과정에서 위와 같은 문제점이 겹쳐있는 객체들에 대한 경계면에 문제가 더욱 커진다고 말하며, disparity를 정수값이 아닌 실제값으로 예측하는 CDN(Continuous Disparity Network)를 소개했다.

앞서 언급한 문제점은 일반적인 disparity 예측에 regression방식을 사용하여 예측과 gt의 오차를 줄이는것을 목표로 하는 방식에서 잘 드러난다. 예를 들어, 70미터 거리의 벽 앞에 30미터 거리의 사람이 서 있는 상황을 생각해 보자. 이때 예측된 depth distribution은 두 가지 class에 대한 30 주변 숫자들과 70 주변 숫자들로 이루어진 multi-modal 분포를 이룰것이다. 단순히 이들값을 모두 평균을 내버린다면 그 값은 30과 70 주변값인 50주변으로 얻어질 것이고, 이렇게 뭉뚱그려진 데이터는 둘 중 그 어느 한쪽과도 정확하게 일치하지 않는다. 따라서 본 논문에서는 각각의 픽셀에 대한 disparity를 예측하고, 그것과 gt의 차이(offset)를 예측하는 sub-network를 소개했다.



## Background

앞선 다른 논문들에서도 정수형태의 픽셀값으로 disparity를 계산하는것을 문제삼으며 해결책을 제시했다. 최근 논문들에서는 정수값의 disparity를 사용하지 않고, disparity후보가 될 수 있는 값들의 확률분포를 이용한다. 이렇게 하기 위해서는 4차원의 disparity feature를 생성해야 한다.

$$\\C_{disp}(u, v, d, :)\\$$

> u : 이미지공간의 가로 좌표
>
> v : 이미지공간의 세로 좌표
>
> d : 오른쪽 이미지와 왼쪽 이미지상 해당 픽셀의 disparity
>
> :  : feature

이때 feature값은 왼쪽, 오른쪽 이미지에서 각각 추출된 feature들을 concatenate한 결과로 사용한다. 이후 이 4차원 feature는 여러 층의 3D conv layer를 지나 각각의 픽셀에 대한 disparity값인 $S_{disp}(u,v,d)$를 만들어낸다. 여기서 disparity차원에 softmax를 적용하면 이 값들을 disparity에 대한 확률분포로 바꿀 수 있다. 

확률분포로 바뀐 $S_{disp}(u,v,d)$값을 가지고 다음과 같은 방법으로 disparity값을 연산하고,

$$\\D(u,v) = \sum_{d} softmax(-S_{disp}(u,v,d)) \times d\\$$

gt와 비교하여 end-to-end로 학습이 가능하다(loss = smooth L1 loss, $\mathcal{A}$ = gt에 포함된 영역).

$$\\\sum_{(u,v) \in \mathcal{A}} loss(D(u,v) - D^{gt}(u,v))\\$$

최근들어 disparity를 사용하지 않고 depth값을 사용하여 $S_{depth}(u,v,z)$를 구성하고, depth map에서 gt와 비교하는 연산을 하기도 한다.

$$\\Z(u,v) = \sum_{d} softmax(-S_{depth}(u,v,z)) \times z \\
\sum_{(u,v) \in \mathcal{A}} loss(Z(u,v) - Z^{gt}(u,v))\\$$

본 논문에서는 disparity나 depth값을 계산하는데에 연속적인 값이 사용되는 것과, loss를 연산하는데 불연속적인 값을 사용하는 것을 지적하고 이 두 과정을 서로 반대되게 진행했다(disparity나 depth에 대해 불연속적인 예측, loss에 대해 연속적 예측).



## 네트워크 구조

본 논문의 네트워크 구조는 다음과 같다.

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/cdn/1.png)

네트워크 구조의 특징은 3D conv layer에서 offset을 예측하여 disparity probability가 가장 높은 값에 offset을 더하거나 빼서 disparity output을 만든다는 점이다. disparity와 depth는 서로 밀접한 관계가 있기 때문에, 본 논문에서는 간략하게 정리하기 위해 disparity에 대해서만 설명한다.

$$\\Disparity(u,v) = {f \times b \over {Depth(u, v)}}\\$$

> f : focal length
>
> b : baseline



### Disparity Estimation

#### Problems

앞서 언급했듯이 본 논문은 bounding box로 예측된 영역 내의 픽셀에 대해 disparity값을 평균내어 사용하는 것을 지적하고 있다.

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/cdn/2.png)

인트로에서 언급한 상황처럼 두 객체가 겹쳐있는 경우의 disparity의 확률분포는 위의 그림과 같이 나타날 것이다. 하지만 이때 모든 값들의 평균을 낸 Mean값을 살펴보면 해당 값에 대한 확률은 0에 가까운것을 알 수 있다. 우리가 예측하고자 하는 값과 거리가 멀다. 또한 "평균값"이라는 것의 물리적인 의미를 살펴보면 그것은 결코 실제 disparity값과 비슷하게 예측될 수 없다.

> 만약 disparity가 10픽셀일 확률이 40%이고, 20픽셀일 확률이 60%라고 해서 해당 disparity값이 16픽셀이라는것은 아니다.



#### Continuous disparity network(CDN)

우리의 목표는 불연속적인 값들에 대한 확률적 접근을 하는것이지만, 다른 모델과 다른점은 불연속적인 값이 정수에 국한되지 않는다는 것이다. 다른 모델들은 아래 식처럼 확률분포의 출력을 만들어낸다.

$$\\p(d|u,v) = \begin{cases}
softmax(-S_{disp}(u,v,d)), & \mbox{if }d \in \mathcal{D} \\
0, & \mbox{otherwise }
\end{cases}\\$$

본 논문은 offset을 예측하는 sub-network를 추가하고, 얻어낸 offset $b(u,v,d)$를 예측에 반영하여 다음과 같은 확률분포 출력을 만들어낸다.

$$\\\tilde{p}(d'|u,v) = \begin{cases}
softmax(-S_{disp}(u,v,d)), & \mbox{when }d' = d + b(u,v,d), d \in \mathcal{D} \\
0, & \mbox{otherwise }
\end{cases}\\$$

이는 임의의값 d'에 대해 Dirac delta 함수가 적용된 다음 식으로 표현될 수 있다.

$$\\\tilde{p}(d' |u, v) = \sum_{d \in \mathcal{D}}p(d|u,v) \delta(d' - (d + b(u, v, d)))\\$$

> $\delta$는 2차원 데이터에서 해당 함수의 x,y값이 같으면 1, 그렇지않으면 0이 되는 함수이다. 이 함수의 인자로 사용된 $d' - (d + b(u,v,d))$를 살펴보면, $d'$과 $d+b(u,v,d)$가 같으면 해당 u, v가 같게되고, 함수의 결과값은 1이 된다. 즉 d'이 d + offset인 경우만 모두 더해서 $\tilde{p}$를 만든다는 뜻이다.

위의 식의 $\tilde{p}(d'|u,v)$분포중 가장 많이 등장하는 값이 $(u,v)$에서의 disparity값으로 결정된다.

그렇다면 계속 등장하는 offset은 어떻게 예측하는 것일까?

논문의 저자는 offset 예측에 대해 G-RMI(논문 : [*Towards Accurate Multi-person Pose Estimation in the Wild*](https://arxiv.org/pdf/1701.01779.pdf))의 pose estimator방식을 사용했다고 한다. 이 논문에서는 offset 예측 과정을 다음과같이 설명했다.

1. 먼저 먼저 box로 예측된 영역의 중앙좌표에서 가로세로 (353x257)의 크기로 이미지를 crop 한다. 

2. ResNet-101을 사용하여 해당 영역의 keypoint에 대한 feature를 뽑아내는데, 이때 keypoint의 수 K = 17에 3을 곱한만큼의 채널을 사용한다. 즉 3 * 17개만큼의 feature를 뽑아낸다.

3. 각각의 keypoint의 좌표를 $l_k$라고 할때, 다음과 같은 조건으로 heatmap을 만든다.
   
   $$\\h_k(x_i) = 1, \mbox{  if  }||x_i - l_k|| \leq R\\$$
   
> 이때 R은 $l_k$을 기준으로하는 원의 반지름이며, 이는 static한 값으로 설정해주는것 같다.
   
4. heatmap을 얻어내고 나면 offset vector는 다음과 같이 계산한다.
   
   $$\\F_k(x_i) = l_k - x_i\\$$
   
5. 네트워크 구조에서 볼 수 있듯이 sub-network와 disparity probability network는 같은 feature를 공유한다.



####  Wasserstein distance

wasserstein distance는 다음과 같이 정의할 수 있다.

$$\\W_p(\mu, \nu) = \left( \underset{\gamma \in \Gamma(\mu, \nu)}{inf}\mathbb{E_\gamma}d(x, y)^p \right)^{1 \over p}\\$$

이 distance는 $\mu, \nu$가 1차원 값에 대한 확률분포일때 문제 해결이 쉬워지는데, 특히 $\nu$가 $y^*$위치에서의 Dirac delta function값이라면 Wasserstein-p distance는 다음과같이 쉽게 정의될 수 있다.

$$\\W_p(\mu, \nu) = (\mathbb{E}_\mu \mathbb{E}_\nu ||x-y||^p)^{1 \over p} = (\mathbb{E}_\mu ||x - y^*||^p)^{1 \over p}\\$$

앞서 구한 $\tilde{p}(d'|u,v)$와 multi-modal 확률분포문제에 대응할 수 있는 gt의 확률분포인 $p^*(d'|u,v)$를 각각 $\mu, \nu$에 대입하면 아래 식처럼 전개할 수 있다.

$$\\W_p(\tilde{p}, p^*) = (\mathbb{E}_{\tilde{p}} || d' - d^*||^p)^{1 \over p} = \left( \sum_{d \in \mathcal{D} }p(d|u,v) ||d + b(u,v,d) - d^* ||^p \right)^{1 \over p} \\= \left( \sum_{d \in \mathcal{D}} softmax(-S_{disp}(u,v,d) || d + b(u,v,d) - d^* ||^p \right)^{1 \over p}\\$$


#### multi-modal ground-truth

앞에서 언급한 multi-modal 확률분포 문제는 예측 하나에 다수의 gt가 대응할 수 있는 경우를 말한다. 즉 뽑아낸 영역 내에 두 개의 객체가 존재하는 경우인데, 이때의 $p^*(d'|u,v)$는 다음과 같은 식으로 정의할 수 있다.

$$\\p^*(d'|u,v) = \sum_{d^* \in \mathcal{D^*}} {1 \over |\mathcal{D^*}|} \delta(d' - d^*)\\$$

> $\mathcal{D^*}$는 픽셀 (u, v)에 대해 gt가 될 수 있는 값들의 집합이다.

이런 경우 $p^*(d'|u,v)$는 Dirac delta function이 아니기 때문에 위의 식과 다른 아래 식을 사용해야 한다.

$$\\W_p(\tilde{p}, p^*) = \left( \int^1 _0 |\tilde{P}^{-1}(x) - P^{*-1}(x)|^p dx \right)^{1 \over p}\\$$

> $\tilde{P}$와 $ P^{*}$는 $\tilde{p}, p^*$들의 누적 분포 함수이다.

p = 1인 경우, 위 식은

$$\\W_1(\tilde{p}, p^*)=\int_{\mathbb{R}} | \tilde{P}(d') - P^*(d')|dd'\\$$

로 정리되어 쉽게 계산할 수 있다.

하지만 실제로 gt에서는 뒷배경 객체의 가려진 부분에 대한 disparity는 얻을 수 없는데, 이는 다음과 같은 규칙으로 만들어내서 사용한다.

각각의 픽셀을 중심으로 (k x k)의 이웃한 픽셀을 설정하고, 중앙 픽셀에는 $\alpha$를 가중치로 사용하여 disparity를 만들고, 남은 주변 픽셀들에는 ${1-\alpha \over k \times k -1}$를 가중치로 사요하여 disparity로 설정한다. 본 논문의 저자는 $k=3, \alpha=0.8$을 사용하여 실험을 진행하였다.



## 실험 내용

CDN은 stereo disparity, stereo depth, 3D object detection with stereo image tasks들에 모두 적용될 수 있고, 각각의 task에 맞는 이전 논문들에 CDN을 적용하여 성능을 비교했다.



### Disparity results

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/cdn/3.png)

> EPE : End-Point-Error, 예측 disparity와 gt간 차이의 평균
>
> PE : k-Pixel Threshold Error, gt에서 k픽셀만큼 벗어난만큼 disparity가 예측되는 경우의 퍼센티지



### 3D od result

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/cdn/4.png)

> RMSE : Root Mean Square Error
>
> ABSR : Absolute Relative Error



## Conclusion

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/cdn/5.png)

위의 사진의 영역에서, 하늘부분에는 disparity가 0에 수렴하기때문에 아랫부분의 disparity와 하늘부분이 같이 평균이 계산되면 GANet의 결과처럼 부정확하게 예측될 수 있다. 하지만 CDN을 사용하게 되면 해당 부분에 대해 background를 잘 걸러내는 것을 볼 수 있다.