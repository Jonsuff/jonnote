---
layout: post
title:  "Xception"
date:   2020-01-26 18:30:13
categories: Deep_Learning
---



# Xception

- *Xception : Deep Learning with Depthwise Separable Convolutions - François Chollet, Google*

Inception에 기초를 두고 발전시킨 모델이다. Inception-V1(GoogLeNet) 모델에서는 입력 데이터에 1x1 convolution filter를 적용하여 노드간 연결을 줄이고 연산량을 줄였지만, Xception에서는 "extreme" version of inception module과 Depth-wise separable convolution 두 종류의 모델을 만들었다.

두 모델의 큰 차이점은 뒤에 설명할 (1)Point-wise convolution의 유무와 (2)ReLU 함수의 유무 이다.

|                        | Extreme version of inception module | Depth-wise separable convolution |
| ---------------------- | ----------------------------------- | -------------------------------- |
| Point-wise convolution | X                                   | O                                |
| ReLU                   | O                                   | X                                |

![An "extreme" version of Inception module](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/Xception_DepthWiseConvolutions.png)

Xception 구조의 마지막 특징은 Depth-wise convolution과 함께 ResNet의 Residual block을 사용하여 skip connection을 추가했다는 점이다. 이때 1x1 convolution에서 stride를 2로 설정하여 입력 데이터의 크기를 반으로 줄이는데, 이는 1x1 convolution 연산이 3x3 Max pooling 뒤에 연결되기 때문이다(Max pooling의 stride는 2이다).



### Depth-wise convolution

기존의 convolution 연산은 다양한 Feature Detector를 사용하여 다수의 채널로서 Feature Map을 뽑아내서 연산을 하는 반면 Depth-wise convolution은 각각의 채널끼리만 convolution 연산을 수행하는 것이다. 

아래 그림을 예시로 Depth-wise convolution에 대해 알아보자.

![Depth-wise convolution example](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/Xception_DepthWiseConvolutionsExample.png)

위와 같이 8x8x3의 입력 이미지를 Depth-wise convolution 하기 위해 사용하는 필터는 3x3x3의 사이즈로 정한다(여기서 filter 사이즈는 자유지만 채널은 입력 데이터와 같아야 한다).

위에서 설명했듯이, 입력데이터의 3가지 채널과 filter의 3가지 채널을 순서에 맞춰서 convolution 연산을 해준 뒤 그 채널의 순서로 출력 데이터가 쌓인것을 볼 수 있다. 이렇게 convolution 연산을 하면 채널 방향의 convolution은 진행하지 않고, 공간 방향의 convolution만 진행하게 된다.



### Point-wise convolution

앞에서 설명한 Depth-wise convolution과 다르게 Point-wise convolution은 공간 방향의 convolution은 진행하지 않고 채널 방향의  convolution만 진행하는 방법이다. 

이 연산은 입력된 데이터에 1x1xc (c = 채널 수)크기의 filter를 사용하여 입력 데이터의 feature를 1개의 채널로 압축 시키는 효과가 있다.

Point-wise convolution의 장점은 채널을 압축시키는 효과가 있기 때문에 연산속도를 크게 향상시킬 수 있다는 것이다. 하지만 데이터가 압축되는 만큼 사라지는 데이터도 존재하기 때문에 연산속도와 데이터 수 사이의 trade-off 관계를 조정하여 연산을 할 필요가 있다.



### Depth-wise separable convolution

Depth-wise convolution을 진행한 결과물에 Point-wise convolution을 진행하는 것을 Depth-wise separable convolution 이라고 한다. 정리해서 말해보면 채널별로 convolution 연산하여 쌓은 결과물에 각각 채널을 1개로 압축할 수 있는 추가 convolution을 진행하여 결과물을 1채널로 줄이는 과정이다. 예시는 아래의 그림과 같으며 연산의 결과물이 아주 간소해 지는 것을 알 수 있다.

![Depth-wise separable convolution example](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/Xception_DepthWiseSeparableConvolutionExample.png)



### ReLU 유무에 따른 결과

해당 논문의 저자들은 실험을 통해 비선형 활성화 함수인 ReLU는 extream version of Inception에 적용했을 때만 성능이 향상됐고, Depth-wise separable convolution에 적용됐을 때에는 오히려 성능이 크게 저하된다는 결과를 도출했다. 



