---
layout: post
title:  "GS3D"
date:   2021-01-03 17:20:00
categories: Deep_Learning
---



# GS3D

| 제목 | GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving |
| ---- | :----------------------------------------------------------: |
| 저자 | Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, Xiaogang Wang  |
| 출판 |                          CVPR 2019                           |

> 저자 정보 : https://libuyu.github.io/



## Introduction

### 3D Object Detection에 사용되는 데이터 종류

3D Object detection은 사용하는 데이터의 종류에 따라 일반적으로 3가지 방식으로 분류된다.

1. LiDAR를 이용한 point cloud 데이터
2. multi-view 이미지 : 비디오, 혹은 stereo 데이터
3. monocular 이미지(~~가장 싸게먹힌다고 강조하고 있다~~)

본 논문은 monocular 이미지를 사용하여 실시간 도로주행 이미지에서 3D 박스를 검출하는 것을 목표로 연구를 진행하였다. 



### 2차원 박스를 이용한 직육면체 추정

스테레오 이미지도 아닌 한 시점의 이미지로 어떻게 3차원 박스를 검출할까? 그 해답은 바로 검출된 2차원 박스를 3차원 직육면체(cuboid)로 추정하는 방법이다. 본 논문의 저자는 2D detection과 해당 scene에 대한 prior knowledge를 이용하여 미리 검출된 2D 박스를 3D 박스로 바꾸는 알고리즘을 소개했다. 이때 얻어진 3D coarse 구조를 *Guidance*라고 부른다. 

Guidance를 이용하여 변환된 3D 박스는 크기(height, width, length)정보 뿐만 아니라 해당 객체의 orientation정보까지 포함한다.



### 3D box refinement

2D 박스에서 3D 박스로 변환하는 것은 차원을 증가시키는 일이므로, 추가적인 정보가 필요하다. 단순히 2D 박스를 3D 박스로 변환시킨다면 아래 그림과 같이 해당 객체의 orientation에 대한 정보가 애매해진다.

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/gs3d/1.png)

이 문제를 해결하기 위해 다음과 같은 단계로 3D box refinement작업을 진행하여 정확한 3D 박스를 얻는다.

1. 2D detection결과와 관측 방향을 이용하여 해당 객체에 대한 coarse basic cuboid(Guidance)를 얻어낸다.
2. 위에서 얻어낸 Guidance를 이용하여 3D 박스의 앞면(visible surfaces)들의 구간으로 나누어 정확한 박스를 검출한다.
3. box refinement는 classification task로 간주하여 기존의 regression방식을 사용하지 않고 classification formulation을 거친 후 quality-aware loss를 사용하여 성능을 높였다.

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/gs3d/3.png)



### 3D 박스 구성

본 논문은 KITTI dataset에서의 3차원 데이터 시스템을 적용하여 3D 박스를 구성하였다. KITTI dataset의 3차원 데이터 시스템의 기본 형식은 다음과 같다.

$$\\B = (w,h,l,x,y,z,\theta,\phi,\psi)\\$$

- $w, h,l$ : width, height, length

- $x,y,z$ : 3D 박스의 **bottom surface center**

- $\theta, \phi, \psi$ : 각각 y, x, z축에 대한 rotation

  -> 본 논문에서는 모든 물체는 땅 위에 있으므로 y축에 대한 회전각도인 $\theta$만 고려



## GS3D

### 네트워크 구조

GS3D의 네트워크 구성은 다음과 같다.

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/gs3d/2.png)

1. **2D+O subnet**

   cnn기반의 2D object detection과 orientation 정보를 얻어내는 subnet을 이용하여 3D guidance를 만든다.

2. 3D guidance를 2차원에 projection 시켜보면 우리 눈에 보이는 앞면(visible surfaces)를 얻어낼 수 있고, 해당 영역의 feature를 추출한다.

3. **3D subnet**

   visible surfaces의 feature들을 3D subnet을 이용하여 refinement를 진행한다. 



### 2D Detection and Orientation

2D+O subnet에 사용되는 모델은 faster R-CNN모델이며, 여기에 orientation prediction 브랜치를 추가한다.

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/gs3d/4.png)

> 선으로 연결되어 있는 것은 FC layer를 뜻한다

추가된 orientation prediction 브랜치에서 예측하는 orientation정보는 객체를 관측하는 시점에서의 각도이다(observation angle of the object). 논문 저자는 이 각도를 글로벌 rotation $\theta$각도와 혼동하지 않기 위해 $\alpha$라고 부른다.

$\theta$와 $\alpha$는 모두 KITTI dataset에 정보가 기재되어 있으며, 둘 사이의 기하학적 관계는 다음 그림과 같다.

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/gs3d/5.png)

파란색 축은 카메라의 관측 축이고, 빨간색 화살표는 자동차 앞부분이 향하는 방향이다. 각도는 시계방향으로 positive방향이다.



### Guidance 생성방법

2D+O subnet의 결과로 얻어낸 2D 박스들을 이용하여 각각에 대응하는 3D 박스를 만든다. 과정을 수식으로 표현해보면

$$\\B^{2D} = (x^{2D}, y^{2D}, h^{2D}, w^{2D})\\ angle = \alpha \\ Mat(camera) = K\\$$

위와 같은 2D 박스 정보와 각도, 카메라 인트린직 K를 이용하여

$\\B_g=(w_g, h_g, l_g, x_g, y_g, z_g, \theta_g)\\$

를 얻어내는 것이다. 



- **Guidance Size** $(w_g, h_g, l_g)$ : 

  자율주행 환경에서 객체의 크기는 클래스마다 크게 다양하지 않고 어느정도 범위 내에서 정형화 되어있다. 따라서 본 논문의 저자는 하나의 클래스마다 정해진 Guidance size $(\bar{w}, \bar{h}, \bar{l})$들을 지정하였다. 따라서 각각의 클래스마다 고유의 $(w_g, h_g, l_g)$를 갖는다.

  

- **Guidance Location** $(x_g, y_g, z_g)$ : 

  앞서 설명한것 처럼 3D 박스의 x, y, z는 bottom surface center값이다. 저자는 다음과 같은 과정을 거쳐 3D박스의 x, y, z를 구한다고 설명한다.

  1. top surface center를 $C(x, y)$, 2D박스의 center를 $M^{2D}(x, y)$라고 한다면 $C$와 $M^{2D}$는 거의 비슷한 좌표값을 갖는다. 이는 데이터를 취득하는 카메라가 자동차의 꼭대기에 달려있고, 자율주행 환경에서의 다른 차들은 대부분 같은 높이의 객체이므로 top surface는 거의 선과 같아보일정도로 찌그러진 사각형이기 때문이다.

     따라서
     
     $$\\box_{2D} = (x^{2D}, y^{2D}, h^{2D}, w^{2D}) \\$$ 라면,
     
     $$\\C_t^{2D} = (M_t^{2D}, 1)= 
     (x^{2D}, y^{2D}+h^{2D} / 2, 1)\\\\$$ 가 성립한다.
     
     
     
  2. bottom surface center를 $C(x, y)$, 2D박스의 center를 $M^{2D}(x, y)$라고 한다면 $C$는 $M^{2D}$보다 비슷하거나 살짝 작은 값을 갖는다. 이는 top surface보다 bottom surface가 상대적으로 더 사각형(평행사변형)에 가깝기 때문에 y값에서 미세한 차이를 보인다.

     따라서 
  
     $$\\C_b^{2D} = 
   (M_t^{2D}, 1) - (0, \lambda h^{2D}, 0) = (x^{2D}, y^{2D} + ({1 \over 2} - \lambda)h^{2D}, 1)\\$$ 이고, 
     
     $\lambda$는 statistics on training data이므로 실험을 통해 자동차에 대한 값은 0.07을 사용했다.
  
  
  
  3. 카메라 intrinsic K를 알고있기 때문에 normalize된 좌표 $\tilde{C}$를 다음과 같이 구할 수 있다.
  
     $$\\\tilde{C_b} = K^{-1}C_b^{2D}\\
   \tilde{C_t} = K^{-1}C_t^{2D}\\$$
     
     depth가 $d$라면 글로벌 Guidance $C_b$는 다음과 같다.
     
     $$\\C_b = d \tilde{C_b}\\$$
     
     최종 목표는 $d$값을 구하는 것으로 정해진 셈이다.
  
     
  
  4. normalize된 top center $\tilde{y_t}$와 bottom center $\tilde{y_b}$를 이용하여 normalize된 높이 $\tilde{h}$를 얻을 수 있다.
  
     $$\\\tilde{y_t} = \tilde{y_b} - \tilde{y_t}\\$$
  
     Guidance height $h_g$는 $\bar{h}$로 클래스마다 이미 정해졌으므로, d는
     
     $$\\d = h_g/\tilde{h} = \bar{h} / \tilde{h}\\$$
     
     결과적으로 Guidance 위치는 다음과 같다. 
     
     $$\\(x_g, y_g, z_g) = C_b = (d\tilde{x_b}, d\tilde{y_b}, d)\\$$
  
  
  
- **Guidance Orientation** $\theta$ :

  앞서 소개한 $\alpha$와 $\theta$사이의 기하학적 관계에 의해서 주어진 $\alpha$에 따라 $\theta$는 다음과 같다.
  
  $$\\\theta_g = \alpha + arctan{x_g \over z_g}\\$$
  
  이때 $x_g, z_g$는 윗단계에서 구했으므로 우리는 $\theta_g$를 구할 수 있다.





### 3D subnet

3D박스를 얻어내고 나면 박스의 visible surface별로 각각의 feature를 추출한다. 항상 객체는 땅 위에 있으므로 bottom surface는 언제나 invisible하고, 자율주행 환경의 cuboid의 특성상 visible surface는 3개가 존재할 수 있다. 3개의 visible surface는 다음과 같은 조건에 의해 결정된다.

1. $\alpha$ > 0 : 

   객체의 앞면이 visible surface에 속하는 조건이다.

2. $\alpha$ < 0 : 

   객체의 뒷면이 visible surface에 속하는 조건이다.

3. $-{\pi \over 2} < \alpha < {\pi \over 2}$ : 

   객체의 오른쪽면이 visible surface에 속하는 조건이다. 이외의 조건이면 객체의 왼쪽면이 visible surface에 속한다.



#### perspective transformation

3가지 visible surfaces에 대한 각각의 feature map은 perspective transformation을 거쳐 (5 x 5) 사이즈로 추출된다. 이때 앞면이던 뒷면이던 가장 크게 앞쪽으로 보이는 visible surface를 $F$라고 부르고, 이에 대해 다음 과정을 거치며 perspective transformation P를 구한다.

1. $F$와 camera projection matrix를 이용하여 $F^{2D}$의 네개의 꼭짓점을 계산한다. 

2. 계산된 $F^{2D}$를 네트워크 stride에 맞추어 feature map에 스케일된 $F_s^{2D}$를 계산한다.

3. $F_s^{2D}$의 네 꼭짓점과 타겟 surface의 feature 네 꼭짓점으로부터 perspective transform matrix P를 얻어낸다.

   $X$가 transform되기 전, $Y$가 transform된 후의 feature map이라고 한다면 다음이 성립한다.
   
   $$\\Y_{i, j} = X_{u, v}\\
   (u, v, 1) = P^{-1}(i, j, 1)\\$$
   
   일반적으로 $(u, v)$는 정수로 딱 떨어지지 않기때문에 bi-linear interpolation을 이용하여 가장 가까운 정수를 찾아내 $X_{u, v}$를 얻어낸다.

   ![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/gs3d/6.png)




#### 구조

자세한 3D subnet의 구조는 다음과 같다.

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/gs3d/7.png)

구조를 자세히 살펴보면 surface features만 사용하는것이 아니라 2D의 bbox feature도 사용하는것을 볼 수 있다.



### Refinement

#### Residual

3D박스의 후보 $(w, h, l, x, y, z, \theta)$와 groud truth $(w^*, h^*, l^*, x^*, y^*, z^*, \theta^*)$를 이용하여 각각의 residual을 지정한다.

$$\\\Delta x = {x^* - x \over \sqrt{l^2 + w^2}}, \ \ 
\Delta y = {y^* - y \over \sqrt{l^2 + w^2}}, \ \ 
\Delta z = {z^* - z \over \sqrt{l^2 + w^2}}\\
\Delta l = log({l^* \over l}), \ \ 
\Delta w = log({w^* \over w}), \ \ 
\Delta h = log({h^* \over h})\\
\Delta \theta = \theta^* - \theta\\$$


#### Classification Formulation

박스의 surface를 찾는 task는 regression보다는 classification 문제에 가깝다고 논문의 저자는 주장한다.

discrete한 regression task를 classification 형태로 바꾸기 위한 key idea는 바로 residual을 여러개의 인터벌로 나눈 후 여러 인터벌중 하나의 인터벌을 고르는 것이다. 다음과 같은 과정으로 진행된다.

1. $d \in \{w, h,l,x,y,z, \theta \}$ 일때 $\Delta d_i = d^{gt}_i - d^{gd}_i$라고 정의한다.
2. $\Delta d$의 표준편차인 $\sigma(d)$를 계산한다.
3. $(0, \pm \sigma(d), \pm 2\sigma(d), ..., \pm N(d)\sigma(d))$를 각 인터벌들의 center값으로 할당한다. 이때 $N(d)$는 $\Delta d$의 범위에 따라 결정된다.
4. 각 인터벌값과 quality값에 대해  binary cross entropy를 계산한다.

이 과정을 거치면서 2D detection에서 발견된 negative박스들을 걸러낼 수 있다고 한다. 그 이유는 실제로 ground truth에 없는 박스들은 그에 대한 Guidance가 annotation에 없으므로 residual range가 0과 가까운 범위내에 있을 것이고, 그렇게 되면 각 인터벌값이 0에 수렴하는 작은 값일 것이다. 따라서 해당 객체는 background로 판명나 2D박스의 negative가 자동으로 걸러지는 것이다.

논문의 저자는 실험을 통해 각 항목의 residual range와 표준편차를 계산했고, 결과적으로 $(w, h, l, y, \theta)$에 대해서 N = 5, $(x, z)$에 대해서 N = 10으로 결정되었다.

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/gs3d/8.png)





### Quality Aware Loss

논문의 저자는 classification 문제에서 더 정확한 박스에 더 높은 score를 부여하고자 했다. 이는 앞서 보여준 그림처럼 차량의 orientation에 따라 3D박스가 잘못 나올수도 있는 경우에는 낮은 score를 부여하기 위함이다.

논문의 저자는 ground truth와 3D박스가 겹치는 정도(ov)에 따라 다음과 같이 quality를 지정했다.

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/gs3d/9.png)

여기서 얻어낸 quality값을 ground truth로, 위에서 언급한 residual range에 대한 인터벌값을 prediction으로 생각하고 binary cross entropy를 진행하여 loss를 얻어낸다.





## Experiments

본 논문은 KITTI object detection 데이터셋을 사용했으며 2D subnet과 3D subnet 모두 backbone은 VGG-16을 사용했다. 



### Guidance Generation

앞서 언급했듯이 w, h, l 파라미터에 대한 Guidance $(\bar{w}, \bar{h}, \bar{l})$는 임클래스마다 지정되어 있다. 본 논문의 저자는 실험에서 차량을 중점적으로 사용했고, 차량 클래스에 대한 Guidance값을 다음과 같이 설정했다.

$$\\\bar{w} = 1.62\\
\bar{h} = 1.53\\
\bar{l} = 3.89\\
\lambda = 0.07\\\\$$


### 검출 결과

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/gs3d/13.png)

위 그림은 본 논문의 코드로 검출한 결과이다. 초록색 박스는 검출에 성공한 경우이고, 맨 아랫 행의 빨간 박스들은 검출에 실패한 결과이다.

왼쪽의 빨간 3D박스는 차량의 orientation을 잘못 검출한 결과이고, 오른쪽의 2D점선박스는 2D detection에서 검출하였으나 classification formulation 과정에서 negative box로 판명난 경우이다. 실패의 예시를 보면 이미지의 가장자리의 객체를 검출하는데 정확도가 떨어지는 것으로 보인다.



### 성능 지표

본 논문에서 사용된 기법은 surface feature, class formulation, quality aware loss가 있다. 이들을 사용 하고 안하고를 비교한 성능 지표는 다음과 같다.

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/gs3d/11.png)



다른 논문과의 비교 테이블은 다음과 같다.

![](https://raw.githubusercontent.com/Jonsuff/jonnote/master/_posts/gs3d/12.png)

