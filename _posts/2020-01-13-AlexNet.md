---
layout: post
title:  "AlexNet"
date:   2020-01-13 20:43:13
categories: Deep_Learning
---



# AlexNet

- 2012년 AlexNet은 이전의 모든 경쟁 업체보다 월등히 뛰어난 CNN 구조이다.
- 기본적인 구조는 LeNet과 매우 유사하지만 레이어당 필터와 스택형 컨볼루션 레이어로 더 깊은 구조를 가지고 있다.
- LeNet과의 가장 큰 차이는 AlexNet은 전체적인 구조로 봤을 때 2개의 GPU를 기반으로 한 병렬 구조인 점이다.
- 5개의 convolution layer와 3개의 fully connected layer로 구성되어 있고, 마지막 FC layer는 1000개의 카테고리로 분류하기 위해 softmax 함수를 활성함수로 사용한다. 

![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/AlexNet_structure.png)

- AlexNet은 약 65만개의 뉴런과 6000만개의 자유 파라미터, 6.3억개의 connection으로 이루어진 방대한 CNN 구조를 가진다.

- ![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/AlexNet_block_image.png)AlexNet의 블록을 표현하는 그림은 위와 같다.

  위의 그림에서 depth는 LeNet에서의 feature map 개수와 같은 의미이고, 각 층의 연산 과정은 동일하다.

  LeNet과의 차이점은 LeNet은 32x32의 1채널 이미지를 사용했지만, AlexNet은 227x227의 3채널 이미지를 사용한다. 

- AlexNet 순서 및 특징

  1. 첫 번째 convolution layer의 kernel 크기는 11x11x3 이고, stride 4로 적용하여 96개의 feature map을 생성한다.

     출력되는 이미지의 크기는 다음 공식에 의해 55개가 된다.
     $$
     out = {Size(Input)-Size(kernel) \over stride}+1 = {227-11 \over 4}+1 = 55
     $$

  2. 첫 번째 convolution layer를 거치면서 GPU-1에서는 주로 컬러와 상관없는 정보를 추출하기 위한 kernel이 학습된다. 

     GPU-2에서는 주로 컬러와 관련된 정보를 추출하기 위한 kernel이 학습된다.

  3. 다음은 Max pooling을 통해 이미지 사이즈를 27x27x96으로 줄인다.

  4. 두 번째 convolution layer는 5x5 사이즈의 kernel로 연산하여 27x27x256의 이미지를 출력하고 다음 Max pooling으로 13x13x256의 사이즈로 축소한다.

  5. 세 번째 convolution layer에서 3x3 사이즈의 kernel로 연산하여 13x13x384의 이미지를 출력한다.

     이 때 두 GPU가 서로의 연산 결과를 섞어서 연산한다.

  6. 네 번째 convolution layer에서 세 번째와 같은 작업을 하지만 이 때 각각 GPU는 연산 결과를 섞지 않는다.

  7. 마지막 convolution layer에서 3x3 사이즈의 kernel로 연산하여 13x13x256의 이미지를 출력하고, Max pooling을 통해 6x6x256으로 사이즈를 축소한다.

  8. convolution layer 연산이 끝나면 두 GPU 결과를 합쳐서 두 층의 fully connected layer로 연산하고 마지막으로 softmax를 통해 1000개의 클래스로 구분된다.



### Rectified Linear Unit (ReLU)

- AlexNet이 2개의 GPU를 사용한 학습방법 이외에 성능, 속도 개선을 위해 사용한 활성화 함수.

- 일반적으로는 sigmoid 함수나 tanh 함수가 활성화 함수로 자주 사용된다. 하지만 AlexNet과 같이 크기가 큰 망에서는 학습 속도가 느려지는 단점이 있다.

- ReLU는 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0보다 작으면 0을 출력한다.

  ![](https://raw.githubusercontent.com/Jonsuff/MLstudy/master/images/ReLU_image.png)
  $$
  h(x) = \begin{cases}x & (x>0) \\ 0 & (x\le 0) \end{cases}
  $$
  

### Overlapping Pooling

- 일반적인 Max pooling은 각각 중복되지 않는 영역에서 pooling 한다.
- AlexNet의 overlapping pooling은 3x3 영역을 2픽셀 단위로 pooling하여 조금씩 겹치는 부분이 있도록 한다. $$\rightarrow$$ overfitting 현상을 개선한다.



### Dropout

- Dropout은 네트워크의 일부를 생략하여 다른 모델을 학습한 것과 같은 효과를 얻어 overfitting을 개선한다.
- AlexNet에서는 첫 번째와 두 번째 fully connected layer에서 Dropout을 적용한다.